{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a022bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from itertools import product\n",
    "import math\n",
    "\n",
    "def get_fingerprint(smiles, r=3, nBits=1024):\n",
    "    compound = Chem.MolFromSmiles(smiles.strip())\n",
    "    fingerprint = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits=nBits)\n",
    "    m = np.zeros((0, ), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fingerprint,m)\n",
    "    return m\n",
    "\n",
    "def sequence_to_kmer(protein_seq, k):\n",
    "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
    "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
    "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
    "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
    "    for i in range(0, len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
    "    ind=[]\n",
    "    for i in range(0, len(crossproduct)): ind.append(i)\n",
    "    combinations=dict(zip(crossproduct, ind))\n",
    "    \n",
    "    V=np.zeros(int((math.pow(7,k))))\n",
    "    try:\n",
    "        for j in range(0, len(protein_seq)-k+1):\n",
    "            kmer=protein_seq[j:j+k]\n",
    "            c=''\n",
    "            for l in range(0, k):\n",
    "                c+=groups[kmer[l]]\n",
    "                V[combinations[int(c)]] += 1\n",
    "    except:\n",
    "        count={'1':0,'2':0,'3':0, '4':0, '5':0,'6':0,'7':0}\n",
    "        for q in range(0,len(protein_seq)):\n",
    "            if protein_seq[q]=='A' or protein_seq[q]=='V' or protein_seq[q]=='G':\n",
    "                count['1']+=1\n",
    "            if protein_seq[q]=='I' or protein_seq[q]=='L'or protein_seq[q]=='F' or protein_seq[q]=='P':\n",
    "                count['2']+=1\n",
    "            if protein_seq[q]=='Y' or protein_seq[q]=='M'or protein_seq[q]=='T' or protein_seq[q]=='S':\n",
    "                count['3']+=1\n",
    "            if protein_seq[q]=='H' or protein_seq[q]=='N'or protein_seq[q]=='Q' or protein_seq[q]=='W':\n",
    "                count['4']+=1\n",
    "            if protein_seq[q]=='R' or protein_seq[q]=='K':\n",
    "                count['5']+=1\n",
    "            if protein_seq[q]=='D' or protein_seq[q]=='E':\n",
    "                count['6']+=1\n",
    "            if protein_seq[q]=='C':\n",
    "                count['7']+=1\n",
    "        \n",
    "        value=list(count.values())\n",
    "        key=list(count.keys())\n",
    "        maximum_occurence=0\n",
    "        index=0\n",
    "        for t in range(0, len(value)):\n",
    "            if maximum_occurence < value[t]:\n",
    "                maximum_occurence = value[t]\n",
    "                index=t\n",
    "        maximum_occurence = key[index] # group number of maximum occuring group\n",
    "        for j in range(0, len(protein_seq)-k+1):\n",
    "            kmer=protein_seq[j:j+k]\n",
    "            c=''\n",
    "            for l in range(0, k):\n",
    "                if kmer[l] not in groups:\n",
    "                    c += maximum_occurence\n",
    "                else:\n",
    "                    c+=groups[kmer[l]]\n",
    "            V[combinations[int(c)]] += 1\n",
    "            \n",
    "        V = V/(len(protein_seq)-1)\n",
    "        return np.array(V)\n",
    "    \n",
    "def get_protein_features(protein_sequence):\n",
    "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    f = []\n",
    "    protein_info = ProteinAnalysis(str(protein_sequence))\n",
    "    protein_info.molecular_weight()\n",
    "    amino_acid_percent = protein_info.get_amino_acids_percent()\n",
    "    dp = []\n",
    "    for a in aa:\n",
    "        dp.append(amino_acid_percent[a])\n",
    "    dp=np.array(dp)\n",
    "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
    "    f.extend(dp[0])\n",
    "    twomer=np.array(sequence_to_kmer(str(protein_sequence), 2))\n",
    "    twomer=normalize(np.atleast_2d(twomer), norm='l2', copy=True, axis=1,return_norm=False)\n",
    "    f.extend(twomer[0])\n",
    "    threemer=np.array(sequence_to_kmer(str(protein_sequence), 3))\n",
    "    threemer=normalize(np.atleast_2d(threemer), norm='l2', copy=True, axis=1,return_norm=False)\n",
    "    f.extend(threemer[0])\n",
    "    return np.array(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a2a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Protein</th>\n",
       "      <th>E3 Target</th>\n",
       "      <th>Warhead</th>\n",
       "      <th>Linker</th>\n",
       "      <th>E3 Ligand</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MGKVNVAKLRYMSRDDFRVLTAVEMGMKNHEIVPGSLIASIASLKH...</td>\n",
       "      <td>MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...</td>\n",
       "      <td>ON1CCC(CC1)NC(=O)C2=C(C=NN2)NC(=O)C3=C(C=CC=C3...</td>\n",
       "      <td>CCCCCCCC</td>\n",
       "      <td>NC1=CC=CC2=C1C(=O)N(C2=O)C3CCC(=O)NC3=O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAVPFVEDWDLVQTLGEGAYGEVQLAVNRVTEEAVAVKIVDMKRAV...</td>\n",
       "      <td>MPRRAENWDEAEVGAEEAGVEEYGPEEDGGEESGAEESGPEESGPE...</td>\n",
       "      <td>OC1=CC2=C(N1)C(=O)N(C=C2C3=C(C=CC(=C3)NS(=O)(=...</td>\n",
       "      <td>COCCOCCOCCN</td>\n",
       "      <td>C(=O)N[C@H](C(=O)N1C[C@@H](C[C@H]1C(=O)NCC2=CC...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSGVSEPLSRVKLGTLRRPEGPAEPMVVVPVDVEKEDVRILKVCFY...</td>\n",
       "      <td>MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...</td>\n",
       "      <td>ON1CCN(CC1)C2=CC=C(C=C2)NC3=NC=C4C(=N3)C(=CS4)...</td>\n",
       "      <td>CCOCCOCCO</td>\n",
       "      <td>C1=C2C(=CC=C1)C(=O)N(C2=O)C3CCC(=O)NC3=O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSSLGASFVQIKFDDLQFFENCGGGSFGSVYRAKWISQDKEVAVKK...</td>\n",
       "      <td>MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...</td>\n",
       "      <td>OC(=O)NC1=CC=CC=C1N</td>\n",
       "      <td>CCCCC</td>\n",
       "      <td>NC1=CC=CC2=C1C(=O)N(C2=O)C3CCC(=O)NC3=O</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MPNYKLTYFNMRGRAEIIRYIFAYLDIQYEDHRIEQADWPEIKSTL...</td>\n",
       "      <td>MPRRAENWDEAEVGAEEAGVEEYGPEEDGGEESGAEESGPEESGPE...</td>\n",
       "      <td>ON1CCN(CC1)CCOC2=CC3=C(C=C2)N4C=C(N=C4S3)C5=CC...</td>\n",
       "      <td>CCCCCCCCCC</td>\n",
       "      <td>C(=O)N[C@H](C(=O)N1C[C@@H](C[C@H]1C(=O)NCC2=CC...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Target Protein  \\\n",
       "0  MGKVNVAKLRYMSRDDFRVLTAVEMGMKNHEIVPGSLIASIASLKH...   \n",
       "1  MAVPFVEDWDLVQTLGEGAYGEVQLAVNRVTEEAVAVKIVDMKRAV...   \n",
       "2  MSGVSEPLSRVKLGTLRRPEGPAEPMVVVPVDVEKEDVRILKVCFY...   \n",
       "3  MSSLGASFVQIKFDDLQFFENCGGGSFGSVYRAKWISQDKEVAVKK...   \n",
       "4  MPNYKLTYFNMRGRAEIIRYIFAYLDIQYEDHRIEQADWPEIKSTL...   \n",
       "\n",
       "                                           E3 Target  \\\n",
       "0  MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...   \n",
       "1  MPRRAENWDEAEVGAEEAGVEEYGPEEDGGEESGAEESGPEESGPE...   \n",
       "2  MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...   \n",
       "3  MAGEGDQQDAAHNMGNHLPLLPAESEEEDEMEVEDQDSKEAKKPNI...   \n",
       "4  MPRRAENWDEAEVGAEEAGVEEYGPEEDGGEESGAEESGPEESGPE...   \n",
       "\n",
       "                                             Warhead       Linker  \\\n",
       "0  ON1CCC(CC1)NC(=O)C2=C(C=NN2)NC(=O)C3=C(C=CC=C3...     CCCCCCCC   \n",
       "1  OC1=CC2=C(N1)C(=O)N(C=C2C3=C(C=CC(=C3)NS(=O)(=...  COCCOCCOCCN   \n",
       "2  ON1CCN(CC1)C2=CC=C(C=C2)NC3=NC=C4C(=N3)C(=CS4)...    CCOCCOCCO   \n",
       "3                                OC(=O)NC1=CC=CC=C1N        CCCCC   \n",
       "4  ON1CCN(CC1)CCOC2=CC3=C(C=C2)N4C=C(N=C4S3)C5=CC...   CCCCCCCCCC   \n",
       "\n",
       "                                           E3 Ligand  Labels  \n",
       "0            NC1=CC=CC2=C1C(=O)N(C2=O)C3CCC(=O)NC3=O       1  \n",
       "1  C(=O)N[C@H](C(=O)N1C[C@@H](C[C@H]1C(=O)NCC2=CC...      -1  \n",
       "2           C1=C2C(=CC=C1)C(=O)N(C2=O)C3CCC(=O)NC3=O       1  \n",
       "3            NC1=CC=CC2=C1C(=O)N(C2=O)C3CCC(=O)NC3=O      -1  \n",
       "4  C(=O)N[C@H](C(=O)N1C[C@@H](C[C@H]1C(=O)NCC2=CC...      -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"filtered_protacs.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07810a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_proteins = np.array(df[\"Target Protein\"])\n",
    "e3_targets = np.array(df[\"E3 Target\"])\n",
    "warheads = np.array(df[\"Warhead\"])\n",
    "linkers = np.array(df[\"Linker\"])\n",
    "e3_ligands = np.array(df[\"E3 Ligand\"])\n",
    "labels = np.array(df[\"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e4e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10758/10758 [00:40<00:00, 268.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10758/10758 [00:24<00:00, 443.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10758, 412)\n",
      "(10758, 412)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp = np.zeros(412)\n",
    "target_protein_features = []\n",
    "for p in tqdm(target_proteins):\n",
    "    tp = get_protein_features(p)\n",
    "    target_protein_features.append(tp)\n",
    "    \n",
    "e3_target_features = []\n",
    "for p in tqdm(e3_targets):\n",
    "    tp = get_protein_features(p)\n",
    "    e3_target_features.append(tp)\n",
    "    \n",
    "e3_target_features = np.array(e3_target_features)\n",
    "target_protein_features = np.array(e3_target_features)\n",
    "\n",
    "print(e3_target_features.shape)\n",
    "print(target_protein_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17a9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10758/10758 [00:05<00:00, 2046.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10758/10758 [00:01<00:00, 5649.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10758/10758 [00:04<00:00, 2627.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of protacs with no linker:  30\n",
      "number of e3 ligands with midway bindings:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "warhead_fingerprints = []\n",
    "for w in tqdm(warheads):\n",
    "    temp = get_fingerprint(w)\n",
    "    warhead_fingerprints.append(temp)\n",
    "    \n",
    "no_linker_count = 0\n",
    "linker_fingerprints = []\n",
    "for l in tqdm(linkers):\n",
    "    if type(l) == float:\n",
    "        temp_array = np.zeros(1024)\n",
    "        linker_fingerprints.append(temp_array)\n",
    "        no_linker_count += 1\n",
    "        continue\n",
    "    temp = get_fingerprint(l)\n",
    "    linker_fingerprints.append(temp)\n",
    "    \n",
    "e3_ligand_fingerprints = []\n",
    "midway_bindings = 0\n",
    "for e in tqdm(e3_ligands):\n",
    "    if \"[R2]\" in e:\n",
    "        e = e.replace(\"[R2]\", \"O\")\n",
    "        midway_bindings += 1\n",
    "    temp = get_fingerprint(e)\n",
    "    e3_ligand_fingerprints.append(temp)\n",
    "    \n",
    "warhead_fingerprints = np.array(warhead_fingerprints)\n",
    "linker_fingerprints = np.array(linker_fingerprints)\n",
    "e3_ligand_fingerprints = np.array(e3_ligand_fingerprints)\n",
    "print(\"number of protacs with no linker: \", no_linker_count)\n",
    "print(\"number of e3 ligands with midway bindings: \", midway_bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5af845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10758, 3896)\n",
      "Epoch  0  loss:  61.75151699781418\n",
      "Epoch  1  loss:  35.80623349547386\n",
      "Epoch  2  loss:  22.594202145934105\n",
      "Epoch  3  loss:  15.222537644207478\n",
      "Epoch  4  loss:  10.10051092505455\n",
      "Epoch  5  loss:  7.559809589758515\n",
      "Epoch  6  loss:  5.591318758204579\n",
      "Epoch  7  loss:  4.769012155011296\n",
      "Epoch  8  loss:  3.963257341645658\n",
      "Epoch  9  loss:  3.6344493981450796\n",
      "Epoch  10  loss:  3.068289832212031\n",
      "Epoch  11  loss:  3.3326302361674607\n",
      "Epoch  12  loss:  3.159964757040143\n",
      "Epoch  13  loss:  2.7190077332779765\n",
      "Epoch  14  loss:  2.756417401600629\n",
      "Epoch  15  loss:  2.819523736834526\n",
      "Epoch  16  loss:  3.0212050625123084\n",
      "Epoch  17  loss:  2.5502129890955985\n",
      "Epoch  18  loss:  2.643561640754342\n",
      "Epoch  19  loss:  2.3957861228846014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 189.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   44\n",
      "200   100\n",
      "300   148\n",
      "400   193\n",
      "500   246\n",
      "600   293\n",
      "700   344\n",
      "800   399\n",
      "900   444\n",
      "1000   500\n",
      "1100   555\n",
      "1200   598\n",
      "1300   655\n",
      "1379   696\n",
      "Accuracy of the network on the protac dataset: 50 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Combine the features together\n",
    "combined_features = np.column_stack((target_protein_features, e3_target_features,\n",
    "                                     warhead_fingerprints, linker_fingerprints,\n",
    "                                     e3_ligand_fingerprints))\n",
    "\n",
    "data = []\n",
    "for i in range(len(combined_features)):\n",
    "    data.append([combined_features[i], labels[i]])\n",
    "        \n",
    "print(combined_features.shape)\n",
    "train_size = 8000\n",
    "val_size = (int) ((len(combined_features)-train_size) / 2)\n",
    "train_dataset = data[:train_size]\n",
    "val_dataset = data[train_size:train_size+val_size]\n",
    "test_dataset = data[train_size+val_size:]\n",
    "\n",
    "\n",
    "# Feed forward Neural network Implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "def cuda(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "\n",
    "def toTensor(v, dtype=torch.float, requires_grad=False):\n",
    "    return cuda(Variable(v.clone().detach()).type(dtype).requires_grad_(requires_grad))\n",
    "\n",
    "def toNumpy(v):\n",
    "    if USE_CUDA:\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()\n",
    "\n",
    "# Hyper Parameters\n",
    "no_of_features = 412*2 + 1024*3\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load datasets\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "\n",
    "# Neural Network with 2 hidden layers\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, no_of_features, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(no_of_features, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "model = cuda(Net(no_of_features, hidden_size, output_size))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "model.train() \n",
    "\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    losses = []\n",
    "    for examples, my_labels in train_loader:\n",
    "        examples = toTensor(examples, dtype=torch.float32)\n",
    "        my_labels = toTensor(my_labels, dtype=torch.float32)\n",
    "        my_labels = my_labels.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(examples)\n",
    "        loss = criterion(output, my_labels)\n",
    "        my_loss = (float) (loss)\n",
    "        losses.append(my_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses = np.array(losses)\n",
    "    print(\"Epoch \", i, \" loss: \", losses.sum())\n",
    "        \n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "threshold = 0\n",
    "for examples, my_labels in tqdm(val_loader):\n",
    "    examples = toTensor(examples, dtype=torch.float32)\n",
    "    output = model(examples)\n",
    "    predictions = []\n",
    "    for o in output:\n",
    "        o = (float) (o)\n",
    "        if threshold > 0:\n",
    "            predictions.append(1.0)\n",
    "        else:\n",
    "            predictions.append(-1.0)\n",
    "    predictions = np.array(predictions)\n",
    "    my_labels = toNumpy(my_labels)\n",
    "    total += len(my_labels)\n",
    "    correct += (predictions == my_labels).sum()\n",
    "    print(total, \" \", correct)\n",
    "    \n",
    "print('Accuracy of the network on the protac dataset: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
